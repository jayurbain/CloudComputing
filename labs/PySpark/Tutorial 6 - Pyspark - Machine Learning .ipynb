{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa0fce",
   "metadata": {},
   "source": [
    "### Pyspark - Machine Learning\n",
    "\n",
    "Jay Urbain, PhD   \n",
    "4/30/2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474b95e",
   "metadata": {},
   "source": [
    "![](spark-cluster-overview.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72615093",
   "metadata": {},
   "source": [
    "# Predicting House Prices with Apache Spark\n",
    "\n",
    "## LINEAR REGRESSION\n",
    "\n",
    "In this lab will apply linear regression to the [California Housing](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) data set to predict house prices using Apache Spark.\n",
    "\n",
    "*Note: This is actually 'small' data and that using Spark in this context is a bit overkill, but transferable to very large datasets.*\n",
    "\n",
    "Submission instructions: Complete all **TODO** items. Submit your copy of this notebook along with feedback on the lab to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3faf497",
   "metadata": {},
   "source": [
    "## 1. The Data Set\n",
    "\n",
    "The California Housing data set appeared in a 1997 paper titled *Sparse Spatial Autoregressions*, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data. Wouldn't you like to buy a house in California for 1990's prices?\n",
    "\n",
    "The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
    "\n",
    "This spatial data contain 20,640 observations on housing prices with 9 economic variables:\n",
    "\n",
    "<p style=\"text-align: justify;\"></p>\n",
    "<pre><strong>Longitude: </strong>refers to the angular distance of a geographic place north or south of the earth’s equator for each block group\n",
    "<strong>Latitude: </strong>refers to the angular distance of a geographic place east or west of the earth’s equator for each block group\n",
    "<strong>Housing Median Age: </strong>is the median age of the people that belong to a block group. Note that the median is the value that lies at the midpoint of a frequency distribution of observed values\n",
    "<strong>Total Rooms: </strong>is the total number of rooms in the houses per block group\n",
    "<strong>Total Bedrooms: </strong>is the total number of bedrooms in the houses per block group\n",
    "<strong>Population: </strong>is the number of inhabitants of a block group\n",
    "<strong>Households: </strong>refers to units of houses and their occupants per block group\n",
    "<strong>Median Income: </strong>is used to register the median income of people that belong to a block group\n",
    "<strong>Median House Value: </strong>is the dependent variable and refers to the median house value per block group\n",
    "</pre>\n",
    "\n",
    "The Median house value is the dependent variable and will be assigned the role of the target variable that we are trying to predict in our ML model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a351735",
   "metadata": {},
   "source": [
    "Make sure Spark can find Java. Set JAVA_HOME to Java in *your* environment path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "JAVA_HOME = \"/Applications/anaconda3/envs/pyspark\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680907ef",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a365413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a385e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caddd01d",
   "metadata": {},
   "source": [
    "Set up visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\n",
    "rcParams['figure.figsize'] = 18,4\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819e324",
   "metadata": {},
   "source": [
    "Set a random seed for notebook reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed=23\n",
    "np.random.seed=rnd_seed\n",
    "np.random.set_state=rnd_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118ca22",
   "metadata": {},
   "source": [
    "## 2. Create the Spark Session\n",
    "\n",
    "Sets the Spark master URL to connect to, such as “local” to run locally, “local[2]” to run locally with 2 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"Linear-Regression-California-Housing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca959e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af25977",
   "metadata": {},
   "source": [
    "SQLContext is a class in Apache Spark that provides a programming interface to work with structured and semi-structured data using SQL \n",
    "\n",
    "It's based on dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880de2f5",
   "metadata": {},
   "source": [
    "## 3. Load The Data From a File Into a Dataframe\n",
    "\n",
    "Download the California housung dataset:\n",
    "\n",
    "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "Set the path to the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d835ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_DATA = 'CaliforniaHousing/cal_housing.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab68cf7",
   "metadata": {},
   "source": [
    "Specifying the schema when loading data into a DataFrame will give better performance than using schema inference.\n",
    "\n",
    "Note: this is what we did in the medical dataset demoed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8aaea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema, corresponding to a line in the csv data file.\n",
    "schema = StructType([\n",
    "    StructField(\"long\", FloatType(), nullable=True),\n",
    "    StructField(\"lat\", FloatType(), nullable=True),\n",
    "    StructField(\"medage\", FloatType(), nullable=True),\n",
    "    StructField(\"totrooms\", FloatType(), nullable=True),\n",
    "    StructField(\"totbdrms\", FloatType(), nullable=True),\n",
    "    StructField(\"pop\", FloatType(), nullable=True),\n",
    "    StructField(\"houshlds\", FloatType(), nullable=True),\n",
    "    StructField(\"medinc\", FloatType(), nullable=True),\n",
    "    StructField(\"medhv\", FloatType(), nullable=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936131a6",
   "metadata": {},
   "source": [
    "Load the data\n",
    "\n",
    "Spark cache is a mechanism that saves a DataFrame (/RDD/Dataset) in the Executors memory or disk. This enables the DataFrame to be calculated only once and reused for subsequent transformations and actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = spark.read.csv(path=HOUSING_DATA, schema=schema).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d0d2f",
   "metadata": {},
   "source": [
    "Inspect the first 5 rows\n",
    "\n",
    "take() and show() are different. show() prints results, take() returns a list of rows (in PySpark) and can be used to create a new dataframe. They are both actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d47f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using take\n",
    "\n",
    "housing_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using show\n",
    "\n",
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff3857",
   "metadata": {},
   "source": [
    "**TODO**: Show the dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show the dataframe columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446da908",
   "metadata": {},
   "source": [
    "**TODO**: Show the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO show the schema of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9575719",
   "metadata": {},
   "source": [
    "## 4. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a sample query using dataframe select\n",
    "\n",
    "housing_df.select('pop','totbdrms').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e235ce04",
   "metadata": {},
   "source": [
    "In order to use SQL, first, create a temporary table on the DataFrame using `createOrReplaceTempView()` function. \n",
    "\n",
    "Once created, this table can be accessed throughout the SparkSession using `sql()` and it will be dropped \n",
    "along with your SparkContext upon termination.\n",
    "\n",
    "Use `sql()` method of the `SparkSession` object to run the query and this method returns a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.createOrReplaceTempView(\"HOUSING\")\n",
    "\n",
    "housing_df_sql = spark.sql(\"SELECT pop, totbdrms from HOUSING\")\n",
    "housing_df_sql.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96645402",
   "metadata": {},
   "source": [
    "### 4.1 Distribution of the median age of the people living in the area:\n",
    "\n",
    "**TODO**: Write a query to generate a histogram of the median age for people living in the area.\n",
    "\n",
    "The query results should look something like the following.\n",
    "\n",
    "```\n",
    "result_df.show(10)\n",
    "\n",
    "+------+--------+\n",
    "|medage|count(1)|\n",
    "+------+--------+\n",
    "|  52.0|    1273|\n",
    "|  36.0|     862|\n",
    "|  35.0|     824|\n",
    "|  16.0|     771|\n",
    "|  17.0|     698|\n",
    "|  34.0|     689|\n",
    "|  26.0|     619|\n",
    "|  33.0|     615|\n",
    "|  18.0|     570|\n",
    "|  25.0|     566|\n",
    "+------+--------+\n",
    "only showing top 10 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987795a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO group by housing median age and see the distribution\n",
    "# your work here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72876b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6170aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e0252f",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65559106",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.toPandas().plot.bar(x='medage',figsize=(14, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e62ed1",
   "metadata": {},
   "source": [
    "### 4.2 Summary Statistics\n",
    "\n",
    "In Spark you can use `df.describe()` or `df.summary()` to review statistical information for numerical columns.\n",
    "\n",
    "The difference is that `df.summary()` returns the same information as `df.describe()` plus quartile information (25%, 50% and 75%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "(housing_df.describe().select(\n",
    "                    \"summary\",\n",
    "                    F.round(\"medage\", 4).alias(\"medage\"),\n",
    "                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n",
    "                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n",
    "                    F.round(\"pop\", 4).alias(\"pop\"),\n",
    "                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n",
    "                    F.round(\"medinc\", 4).alias(\"medinc\"),\n",
    "                    F.round(\"medhv\", 4).alias(\"medhv\"))\n",
    "                    .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9803b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(housing_df.summary().select(\n",
    "                    \"summary\",\n",
    "                    F.round(\"medage\", 4).alias(\"medage\"),\n",
    "                    F.round(\"totrooms\", 4).alias(\"totrooms\"),\n",
    "                    F.round(\"totbdrms\", 4).alias(\"totbdrms\"),\n",
    "                    F.round(\"pop\", 4).alias(\"pop\"),\n",
    "                    F.round(\"houshlds\", 4).alias(\"houshlds\"),\n",
    "                    F.round(\"medinc\", 4).alias(\"medinc\"),\n",
    "                    F.round(\"medhv\", 4).alias(\"medhv\"))\n",
    "                    .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e20329",
   "metadata": {},
   "source": [
    "Notice that multiple attributes have a wide range of values. We will need to normalize the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c814121",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "+ We shouldn't need to care about missing values; all zero values have been excluded from the data set.\n",
    "+ We should definitely standardize our data, as we have seen that the range of minimum and maximum values is quite big.\n",
    "+ There are possibly some additional attributes that we could add, such as a feature that registers the number of bedrooms per room or the rooms per household.\n",
    "+ Our dependent variable is also quite large. To make our life easier, we'll have to adjust the values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06441d48",
   "metadata": {},
   "source": [
    "### 5.1 Preprocessing The Target Values\n",
    "\n",
    "`medianHouseValue` is our dependent variable (target). To make things a little easier we will express the house values in units of 100,000. That means that a target such as `452600.000000` should become `4.526`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b70e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the values of median house value\n",
    "housing_df = housing_df.withColumn(\"medhv\", col(\"medhv\")/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b04172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 2 lines of `df`\n",
    "housing_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc2d959",
   "metadata": {},
   "source": [
    "We can  see that the values have been adjusted correctly when we look at the result of the `show()` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155fa02",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Now that we have adjusted the values for `medhv`, we will add the following columns to the data set:\n",
    "\n",
    "+ Rooms per household which refers to the number of rooms in households per block group. `totrooms/houshlds`\n",
    "+ Population per household, which basically gives us an indication of how many people live in households per block group. `pop/houshlds`\n",
    "+ Bedrooms per total rooms which will give us an idea about how many rooms are bedrooms per block group. `totbdrms/totrooms`\n",
    "\n",
    "We can add columns to a DataFrame based on another existing column value using `df.withColumn`\n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html\n",
    "\n",
    "\n",
    "The results should look as follows:\n",
    "\n",
    "```\n",
    "+--------+--------+----+\n",
    "|rmsperhh|rmsperhh|  as|\n",
    "+--------+--------+----+\n",
    "|    6.98|    2.56|0.15|\n",
    "|    6.24|    2.11|0.16|\n",
    "|    8.29|     2.8|0.13|\n",
    "|    5.82|    2.55|0.18|\n",
    "|    6.28|    2.18|0.17|\n",
    "+--------+--------+----+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87067383",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2992aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new columns to `df`\n",
    "housing_df = (housing_df.withColumn(\"rmsperhh\", F.round(col(\"totrooms\")/col(\"houshlds\"), 2))\n",
    "                       .withColumn(\"popperhh\", F.round(col(\"pop\")/col(\"houshlds\"), 2))\n",
    "                       .withColumn(\"bdrmsperrm\", F.round(col(\"totbdrms\")/col(\"totrooms\"), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924f98c9",
   "metadata": {},
   "source": [
    "Looking at the first row, there are about 6.98 rooms per household, the households in the block group consist of about 2.6 people and the amount of bedrooms is quite low with 0.15:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a404a",
   "metadata": {},
   "source": [
    "Since we don't want to  standardize our target values, we need to isolate the target value in our data set. \n",
    "\n",
    "Note: this is also the time to leave out variables that we might not want to consider in our analysis. Let's leave out variables such as longitude, latitude, housingMedianAge and totalRooms.\n",
    "\n",
    "In this case, we will use the `select()` method and pass the column names in an order that is more appropriate. In this case, the target variable medianHouseValue is put first, so that it won't be affected by the standardization.\n",
    "\n",
    "**TODO**: Select and order the columns in the dataframe as follows:\n",
    "\n",
    "```\n",
    "\"medhv\", \n",
    "\"totbdrms\", \n",
    "\"pop\", \n",
    "\"houshlds\", \n",
    "\"medinc\", \n",
    "\"rmsperhh\", \n",
    "\"popperhh\", \n",
    "\"bdrmsperrm\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533035bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Re-order and select columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd1f38",
   "metadata": {},
   "source": [
    "### 6.1 Feature Extraction\n",
    "\n",
    "Now that we have re-ordered the data, we're ready to normalize the data. Choose the features to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c93d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols = [\"totbdrms\", \"pop\", \"houshlds\", \"medinc\", \"rmsperhh\", \"popperhh\", \"bdrmsperrm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889966a",
   "metadata": {},
   "source": [
    "Use a VectorAssembler to put features into a feature vector column.\n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put features into a feature vector column\n",
    "assembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe41596",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_df = assembler.transform(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87924616",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c936cd3",
   "metadata": {},
   "source": [
    "All of the features should have been transformed into a Dense Vector. Notice the features column above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634dadf",
   "metadata": {},
   "source": [
    "### 6.2 Standardization\n",
    "\n",
    "We can now scale the data using `StandardScaler`. The input columns are the `features`, and the output column are rescaled features that we will be include in the `scaled_df` named `\"features_scaled\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the DataFrame to the scaler\n",
    "scaled_df = standardScaler.fit(assembled_df).transform(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f49272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result\n",
    "scaled_df.select(\"features\", \"features_scaled\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352548fd",
   "metadata": {},
   "source": [
    "### 7. Building A Machine Learning Model With Spark ML\n",
    "\n",
    "With preprocessing done, we can finally build the Linear Regression model. \n",
    "\n",
    "First, split the data into training and test sets using `randomSplit()`:\n",
    "\n",
    "Pass in a list with two numbers that represent the size that we want your training and test sets to have and a seed, which is needed for reproducibility reasons.\n",
    "\n",
    "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.randomSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659315f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2], seed=rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80282d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52507ec7",
   "metadata": {},
   "source": [
    "**Create an ElasticNet model:**\n",
    "\n",
    "`ElasticNet` is a linear regression model trained with `L1` and `L2` prior as a regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like `Lasso`, while still maintaining the regularization properties of `Ridge` regression. We can control the convex combination of `L1` and `L2` using the `l1_ratio parameter`.\n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\n",
    "\n",
    "The objective function to minimize is in this case:\n",
    "\\begin{align}\n",
    "min_w\\frac{1}{2n_{samples}}{\\parallel{X_w - y}\\parallel}^2_2 + \\alpha\\lambda{\\parallel{X_w - y}\\parallel}_1 + \\frac{\\alpha(1-\\lambda)}{2}{\\parallel{w}\\parallel}^2_2\n",
    "\\end{align}\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#elastic-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cffdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize `lr`\n",
    "lr = (LinearRegression(featuresCol='features_scaled', labelCol=\"medhv\", predictionCol='predmedhv', \n",
    "                               maxIter=10, regParam=0.3, elasticNetParam=0.8, standardization=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c40078",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Model\n",
    "\n",
    "With our model in place, we can generate predictions for our test data. \n",
    "\n",
    "We can use the `transform()` method to predict the labels for the `test_data`. Then, we can use RDD operations to extract the predictions as well as the true labels from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients for the model\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc872e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept for the model\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535dcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame({\"Feature\": [\"Intercept\"] + featureCols, \"Co-efficients\": np.insert(linearModel.coefficients.toArray(), 0, linearModel.intercept)})\n",
    "coeff_df = coeff_df[[\"Feature\", \"Co-efficients\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690ecd6",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "Inspect the coefficients. Which independent variable is most predictive?\n",
    "\n",
    "Come up with a working hypothesis for why you think this is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443d453",
   "metadata": {},
   "source": [
    "### 8.2 Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predictions and the \"known\" correct labels\n",
    "predandlabels = predictions.select(\"predmedhv\", \"medhv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "predandlabels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1fb8d",
   "metadata": {},
   "source": [
    "### 8.3 Inspect the Metrics\n",
    "\n",
    "We need to calculate some metrics to get a better idea of how good your model actually is.\n",
    "\n",
    "**TODO**: Use the `LinearRegressionModel.summary` attribute:** to calculate RMSE, MAE, and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE\n",
    "print(\"RMSE: {0}\".format( ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE: {0}\".format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the R2\n",
    "print(\"R2: {0}\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8e8a3",
   "metadata": {},
   "source": [
    "## 9.0 Try to improve the model\n",
    "\n",
    "**TODO** Try to improve the model based on RMSE, MAE, or R2 using any means. \n",
    "\n",
    "For example, additional feature engineering such as adding back in the longitude, latitude, housingMedianAge and totalRooms; trying different alpha and lambda elastic regression coefficients (http://scikit-learn.org/stable/modules/linear_model.html#elastic-net), or using a different machine learning model (https://spark.apache.org/docs/2.3.0/api/python/pyspark.ml.html).\n",
    "\n",
    "Great work on this will be rewarded with extra credit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf99fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
